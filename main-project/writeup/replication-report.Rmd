---
title: "Replication of Study 2 by Walsh & Sloman (2004)"
author: |
  | Erin Bennett
  | `erindb@stanford.edu`
output:
  html_document:
    toc: false
    highlight: zenburn
    toc_depth: 3
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, cache=TRUE, message=FALSE, sanitiz =TRUE)
```

```{r}
library(tidyr)
library(dplyr)
library(pwr)
library(jsonlite)
library(ggplot2)
library(ggthemes)
theme_new <- theme_set(theme_few())

# for bootstrapping 95% confidence intervals
theta <- function(x,xdata) {mean(xdata[x])}
ci.low <- function(x) {
  quantile(bootstrap::bootstrap(1:length(x),1000,theta,x)$thetastar,.025)}
ci.high <- function(x) {
  quantile(bootstrap::bootstrap(1:length(x),1000,theta,x)$thetastar,.975)}
```

# Introduction

This study is about how people revise their beliefs when they are presented with a counterexample to a causal relationship.

This research addresses a few questions:

1. **Are other effects screened off from the effect in the counterexample, given that the cause happened?** That is, do people update their beliefs about that particular causal relationship in isolation, without influencing other parts of their theory? Or do they update their theory in a more complex way such that in this and other counterexamples, other things that usually result from the cause are also less likely? E.g. "My new neighbors tell me they are in a fraternity. I think they will have loud parties late at night and their apartment will be messy. If after a few weekends, the apartment is always really quiet, my expectation might be less strong that it's messy."

    Walsh & Sloman found that effects are not screened off and that the counterexample results in people expecting other possible consequents less.
    
    The main comparison that I will consider a replication is Comparison 1.
    
    *Comparison 1:* For "A causes B" and "A causes C", ask participants about two situations:
    
    1) A is true. How likely is C? (baseline)
    
    2) A is true and B is false. How likely is C? (counterexample)
    
    If B and C are screened off from one another given A, then both questions should have the same answer. If participants are revising their theories in a more comples way, they might be different. In fact, the answer for 2 tends to be lower than the answer for 1.

<!-- 2. **Are other effects screened off, given that people's explanation of the counterexample doesn't hold?** That is, do people revise their beliefs by hypothesizing one explanation for the counterexample (the cause doesn't really hold, something happened to prevent the effect, some necessary condition wasn't met, etc.) and using that explanation to make new predictions? E.g. "Maybe they're in a really nerdy fraternity. If another fraternity replaces them that isn't nerdy, the new fraternity will probably have loud parties and messy apartments."

    Walsh & Sloman found that other effects *are* screened off given the cause is true *and* the explanation the participant generated for the counterexample is false.
    
    *Comparison 2:* Ask participants about two situations:
    
    1) A is true and B is false.
        a. Why do you think that is?
        b. Also, how likely is C?
        
    2) A is true and whatever explanation you gave for the situation in 1 is true. How likely is C?
    
    If B and C are screened off from one another given the explanation for the counterexample, then both questions should have the same answer. In fact, they do. -->

2. **Do people revise their causal believes in the face of a counterexamples?** That is, are these exceptions, conditions, etc. already known and accounted for and participants just have enough evidence for them when they see the counterexample? Or are they adding them in as needed? E.g. "If a *new fraternity* replaces my neighbors that may or may not be nerdy (I just don't know), then will my experience with my quiet neighbors influence my expectations about the new fraternity?"

    Walsh & Sloman found that people have report different probabilities of effects given causes for (almost) the same question after the counterexample.

    *Comparison 2:* Ask participants about P(C|A) before and after the counterexample.
    
    1) A is true. How likely is C? (baseline)
        
    2) A is true and you don't know whether that explanation you gave for the counterexample situation is true. How likely is C? (generalization)
    
    If beliefs are not revised, you would expect no difference between these responses, because 2 has no more information than 1 (literally, at least). But in fact, responses to these questions tend to be different.

# Methods

The replication experiment can be found at this link: [http://erindb.me/254/main-project/](http://erindb.me/254/main-project/).

## Power analysis

Original N=20.
Planned N=20.

```{r, echo=F, warning=F, message=F}
orig.d = data.frame(
  question = c('B|A', 'B|A and not C', 'B|A and explanation unknown', 'B|A and explanation absent', 'B|A and explanation present'),
  qnum = c(0, 2, 3, 4, 5),
  prob = c(0.85, 0.63, 0.71, 0.85, 0.62),
  n = 20
)
orig.rs = data.frame(
  comparison = c(1,2,3),
  qnA = c(0, 2, 0),
  qnB = c(2, 5, 3),
  questions = c('q0>q2', 'q2~q5', 'q0>q3'),
  tvalue = c(5.03, 0.60, 5.37),
  n = 20
) %>% mutate(df = n-1,
             p = pt(tvalue, df),
             es = tvalue/sqrt(n))
orig.rs$power = mapply(function(n, d) {return(pwr.t.test(n=n, d=d, sig.level=0.05, type='paired')$power)}, orig.rs$n, orig.rs$es)
orig.rs$expectedES = mapply(function(n, power) {return(pwr.t.test(n=n, power=power, sig.level=0.05, type='paired')$d)}, orig.rs$n, orig.rs$power)
```

Effect size for the first comparison -- P(B|A) versus P(B|A and not C) -- was 1.12, so with the same sample size the power will be 0.999.

Effect size for the second comparison -- P(B|A) versus P(B|A and explanation unknown) -- was 1.20, so with the same sample size the power will be 0.999.

## Planned sample

English-speaking participants on Amazon's Mechanical Turk.

## Materials

Walsh & Sloman (2004) report using 6 different stories. Two are used as examples in their 2004 paper. <!-- Two more are used as an example in a 2008 report of the same experiment in a book chapter. -->

Stories:

* Jogging regularly causes a person to lose weight. Jogging regularly causes a person to increase their fitness level.
* Worrying causes difficulty in concentrating. Worrying causes insomnia.
<!-- * Following this diet causes you to have a good supply of iron. Following this diet causes you to lose weight.
* Playing loud music in Paul’s apartment causes the neighbors on his left to complain. Playing loud music in Paul’s apartment causes the neighbors on his right to increase the volume on their TV. -->

These stories were (I think) designed such that the effects would not directly affect one another, though I'm not sure they succeeded. I created seven more items with this intended causes structure.

* Drinking coffee causes a person to stay up late. Drinking coffee causes a person to be more focused.
* Going to the beach on a sunny day causes a person to get a suntan. Going to the beach on a sunny day causes a person to see surfers.
* Reading for a very long stretch of time causes a person to get a headache. Reading for a very long stretch of time causes a person's vocabulary to increase.
* Joining a cooking meetup group causes a person to make friends. Joining a cooking meetup group causes a person to get better at cooking.
* Taking a painting class causes a person to get paint on their clothes. Taking a painting class causes a person to improve their painting skills.
* Practicing guitar a lot causes a person to start writing their own songs. Practicing guitar a lot causes a person's fingers to get calloused.
* Flying from the US to Japan causes a person to experience jetlag. Flying from the US to Japan causes a person to hear more Japanese.

## Procedure

Each participant sees each story. Stories are presented one at a time, in blocks.

In each block, first the causal statements in question are presented. These will be visible for rest of the block. Then the following questions are asked (in order):

1) A is true. How likely is C? (baseline)

2) A is true and B is false.
    a. Why do you think that is? (counterexample)
    b. Also, how likely is C?
    
3) A is true and you don't know whether *[the explanation you gave for 2a]* is true. How likely is C? (generalization)

4) A is true and *[the explanation you gave for 2a]* is false. How likely is C? (explanation absent)

5) A is true and *[the explanation you gave for 2a]* is true. How likely is C? (explanation present)

## Analysis plan

Run t-tests to see whether the following replicate:

* *Comparison 1:* $Answer2b < Answer1$
* *Comparison 2:* $Answer1 \neq Answer3$

## Differences from original study

The sample population of the original study was a group of college students. My sample will be participants on Amazon's Mechanical Turk. Both groups probably reason similarly about causes. However, it is possible that workers on Mechanical Turk will be more concerned about giving consistent answers than college students, so for similarly phrased answers we might expect a slight bias for this new population giving the same answer.

The setting will be online rather than in a lab, which means that participants might be paying less attention, or take breaks in the middle of the study. I don't anticipate that having a strong effect on the results, since these questions don't seem to require a lot of concentration.

<!-- attention checks!!! -->

I will use some basic natural language processing to repeat back participants' explanations with (hopefully) appropriate syntax. The original study used a human to do this task. Because doing this with a computer is more error-prone, I will have to ask the question in a slightly different way, giving participants a particular syntactic frame to fill in. The resulting program might respond to people with nonsense. Therefore, I also provide a checkbox for participants to mark if they "notice something strange about the way this question is worded".

# Methods Addendum

## Actual sample

## Differences from pre-date collection methods plan

# Results

## Data preparation

```{r echo=F, message=F, warning=F}

raw.data.path <- "../../data-collection/sandbox-results/"

workers = c()

d <- data.frame()
files <- dir(raw.data.path,pattern="*.json")

for (file.name in files) {

  json_file <- readLines(paste(raw.data.path,file.name,sep=""))
  json_file_str = paste(json_file, collapse = "")
  json_file_str = gsub(",}", "}", json_file_str)
  jso = jsonlite::fromJSON(json_file_str)
  trials = jso$answers$data$trials
  if (jso$WorkerId %in% workers) { 
    trials$workerid = which(workers == jso$WorkerId)
  } else {
    workers = c(workers, jso$WorkerId)
    trials$workerid = which(workers == jso$WorkerId)
  }
  demographics = jso$answers$data$demographics
  for (i in 1:length(demographics)) {
    trials[demographics[i,'qtype']] = demographics[i, 'response']
  }
  events = jso$answers$data$events
  jso1 <- data.frame(trials)

  d <- rbind(d, jso1)
}
```

## Confirmatory analysis

```{r echo=F, message=F, warning=F}
num = function(x) {return(as.numeric(as.character(x)))}
answer1 = d %>% filter(qtype == 'probability', qnum == 0) %>% select(workerid, story, response)
answer2b = d %>% filter(qtype == 'probability', qnum == 2) %>% select(workerid, story, response)
answer3 = d %>% filter(qtype == 'probability', qnum == 3) %>% select(workerid, story, response)
stopifnot(answer2b$story == answer3$story)
stopifnot(answer1$story == answer3$story)
stopifnot(answer2b$workerid == answer3$workerid)
stopifnot(answer1$workerid == answer3$workerid)
answers = data.frame(workerid = answer1$workerid,
                     story = answer1$story,
                     answer1 = num(answer1$response),
                     answer2b = num(answer2b$response),
                     answer3 = num(answer3$response))
comp1 = with(answers, t.test(answer1, answer2b))
comp2 = with(answers, t.test(answer1, answer3))
replication.n = length(unique(d$workerid))
mean.probs = rbind(d %>% filter(qnum %in% c(0, 2, 3, 4, 5)) %>%
  mutate(question = factor(as.character(qnum),
                           levels = c('0', '2', '3', '4', '5'),
                           labels = c('baseline', 'counterexample',
                                      'generalization', 'explanation absent',
                                      'explanation present')),
         probability = num(response)) %>%
  group_by(question) %>%
  summarise(mean.rating = mean(probability),
            cil = ci.low(probability), 
            cih = ci.high(probability),
            version = 'replication'),
  data.frame(question = c('baseline', 'counterexample',
                                      'generalization', 'explanation absent',
                                      'explanation present'),
             mean.rating = c(.85, .63, .71, .85, .62),
             cil = c(.85, .63, .71, .85, .62),
             cih = c(.85, .63, .71, .85, .62),
             version = 'original'))
```
    
The main comparison that I will consider a replication is Comparison 1.

We ran a paired t-test on the comparison between question 1 (baseline) and question 2b (counterexample). In a pilot with `r replication.n` non-naive participants, we do not find evidence that people revise their causal beliefs about other aspects of a particular situation (t(`r comp1$parameter`)=`r comp1$statistic`, p=`r comp1$p.value`).

```{r, fig.width=5, fig.height=3}
mycolors = c('steelblue', 'coral', 'goldenrod1', 'forestgreen', 'violet')
mean.probs %>% filter(question %in% c('baseline', 'counterexample'))  %>%
  ggplot(., aes(x=question, y=mean.rating, colour=question, fill=question)) +
  geom_bar(stat='identity', alpha=1/3) +
  ylim(0, 1) +
  xlab('condition') +
  ylab('average probability rating') +
  geom_errorbar(width=0.1, aes(ymin=cil, ymax=cih, x=question)) +
  scale_colour_manual(values=mycolors) +
  scale_fill_manual(values=mycolors) +
  theme_few() +
  facet_grid(~version) +
  theme(axis.text.x = element_text(angle = -35, hjust = 0))
```

A secondary comparison is the comparison between question 1 (baseline) and question 3 (generalization).

We again ran a paired t-test and did not find evidence that these participants generalize this revision to other situations (t(`r comp2$parameter`)=`r comp2$statistic`, p=`r comp2$p.value`).

```{r, fig.width=5, fig.height=3}
mycolors = c('steelblue', 'coral', 'goldenrod1', 'forestgreen', 'violet')
mean.probs %>% filter(question %in% c('baseline', 'generalization'))  %>%
  ggplot(., aes(x=question, y=mean.rating, colour=question, fill=question)) +
  geom_bar(stat='identity', alpha=1/3) +
  ylim(0, 1) +
  xlab('condition') +
  ylab('average probability rating') +
  geom_errorbar(width=0.1, aes(ymin=cil, ymax=cih, x=question)) +
  scale_colour_manual(values=c('steelblue', 'goldenrod1', 'forestgreen', 'violet')) +
  scale_fill_manual(values=c('steelblue', 'goldenrod1', 'forestgreen', 'violet')) +
  theme_few() +
  facet_grid(~version) +
  theme(axis.text.x = element_text(angle = -35, hjust = 0))
```

Qualitatively, we found similar results in the pilot replication with non-naive participants as in the original study.

```{r, fig.width=10, fig.height=6}
mycolors = c('steelblue', 'coral', 'goldenrod1', 'forestgreen', 'violet')
mean.probs %>% 
  ggplot(., aes(x=question, y=mean.rating, colour=question, fill=question)) +
  geom_bar(stat='identity', alpha=1/3) +
  ylim(0, 1) +
  xlab('condition') +
  ylab('average probability rating') +
  geom_errorbar(width=0.1, aes(ymin=cil, ymax=cih, x=question)) +
  scale_colour_manual(values=mycolors) +
  scale_fill_manual(values=mycolors) +
  theme_few() +
  facet_grid(~version) +
  theme(axis.text.x = element_text(angle = -35, hjust = 0))
```

## Exploratory analyses

# Discussion

## Summary of replication attempt

## Commentary